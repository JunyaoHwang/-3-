

### 13.1 循环神经网络

---

Elman

---

循环神经网络 (RNN) 指任何在网络连接中包含环路的网络，意味着某些单元的值直接或间接依赖于其先前输出作为输入。尽管功能强大，但此类网络难以推理和训练。然而，在循环网络的一般类别中存在一些受限结构，在应用于语言时被证明极为有效。本节考虑一类称为 Elman 网络 (Elman, 1990) 或简单循环网络(simple recurrent networks)的循环网络。这些网络自身就很有用，并作为后文讨论的长短期记忆 (LSTM) 网络等更复杂方法的基础。本章中当我们使用 RNN 一词时，指的就是这些更简单、更受限的网络(尽管你经常会看到 RNN 用来指代任何具有循环特性的网络，包括 LSTM)。

图13.1 展示了 RNN 的结构。与普通前馈网络一样，表示当前输入的输入向量 ${\mathbf{x}}_{t}$ 先与权重矩阵相乘，然后通过非线性激活函数以计算隐藏单元层的值。该隐藏层随后用于计算相应的输出 ${\mathbf{y}}_{t}$ 。与此前基于窗口的方法不同，序列通过一次向网络呈现一项来处理。我们将用下标表示时间，因此 ${\mathbf{x}}_{t}$ 将表示时间 $t$ 的输入向量 $\mathbf{x}$ 。与前馈网络的关键区别在于图中用虚线示出的循环连接。该连接在隐藏层的计算输入中增加了来自前一时间点隐藏层的值。

![bo_d55a0kv7aajc73fvg5i0_13_782_324_253_194_0.jpg](images/bo_d55a0kv7aajc73fvg5i0_13_782_324_253_194_0.jpg)

图13.1 Elman (1990) 的简单循环神经网络。隐藏层包含作为其输入一部分的循环连接。也就是说，隐藏层的激活值取决于当前输入以及来自前一时间步的隐藏层激活值。

前一时间步的隐藏层提供了一种记忆或上下文，编码了早期的处理并影响随后时间点的决策。关键是，这种方法不对先前上下文施加固定长度的限制；先前隐藏层所体现的上下文可以包含追溯到序列开头的信息。

加入这个时间维度使 RNN 看起来比非循环架构更复杂。但实际上并没有太大不同。给定一个输入向量和前一时间步隐藏层的值，我们仍在执行第6章引入的标准前馈计算。为说明这一点，参见图13.2，它澄清了循环的本质以及它如何参与隐藏层的计算。最显著的变化在于一组新的权重 $\mathbf{U}$，它们将前一时间步的隐藏层连接到当前隐藏层。这些权重决定了网络在计算当前输入输出时如何利用过去的上下文。与网络中的其他权重一样，这些连接通过反向传播进行训练。

![bo_d55a0kv7aajc73fvg5i0_13_439_1389_935_354_0.jpg](images/bo_d55a0kv7aajc73fvg5i0_13_439_1389_935_354_0.jpg)

图13.2 将简单循环神经网络示为前馈网络。来自先前时间步的隐藏层 ${\mathbf{h}}_{t - 1}$ 与权重矩阵 $\mathbf{U}$ 相乘，然后加到当前时间步的前馈分量上。

#### 13.1.1 RNN 中的推理

在 RNN 中的前向推理(将输入序列映射到输出序列)几乎与我们在前馈网络中看到的相同。为了为输入 ${\mathbf{x}}_{t}$ 计算输出 ${\mathbf{y}}_{t}$，我们需要隐藏层 ${\mathbf{h}}_{t}$ 的激活值。为计算该值，我们将输入 ${\mathbf{x}}_{t}$ 与权重矩阵 $\mathbf{W}$ 相乘，将前一时间步的隐藏层 ${\mathbf{h}}_{t - 1}$ 与权重矩阵 $\mathbf{U}$ 相乘。将这些值相加并通过合适的激活函数 $g$，以得到当前隐藏层 ${\mathbf{h}}_{t}$ 的激活值。一旦得到隐藏层的值，我们就按常规计算生成输出向量。

$$
{\mathbf{h}}_{t} = g\left( {{\mathbf{{Uh}}}_{t - 1} + \mathbf{W}{\mathbf{x}}_{t}}\right) \tag{13.1}
$$

$$
{\mathbf{y}}_{t} = f\left( {\mathbf{{Vh}}}_{t}\right) \tag{13.2}
$$

我们把输入层、隐藏层和输出层的维度分别记为 ${d}_{in},{d}_{h}$、${d}_{\text{ out }}$。由此，我们的三个参数矩阵为:$W \in  {\mathbb{R}}^{{d}_{h} \times  {d}_{in}},\mathbf{U} \in  {\mathbb{R}}^{{d}_{h} \times  {d}_{h}}$、和 $\mathbf{V} \in  {\mathbb{R}}^{{d}_{\text{ out }} \times  {d}_{h}}$。

我们通过 softmax 计算得出 ${y}_{t}$，该计算为可能的输出类别给出一个概率分布。

$$
{\mathbf{y}}_{t} = \operatorname{softmax}\left( {\mathbf{{Vh}}}_{t}\right) \tag{13.3}
$$

事实是，在时间 $t$ 的计算需要时间 $t - 1$ 的隐藏层值，这就要求一种从序列开始到结束的增量推断算法，如图 13.3 所示。通过将网络在时间上展开(见图 13.4)，也可以看出简单循环网络的序列性。在该图中，各时间步的各层单元被复制以示其随时间变化的不同取值。然而，各权重矩阵在时间上是共享的。

function FORWARDRNN $\left( {\mathbf{x},\text{ network }}\right)$ 返回 输出序列 $\mathbf{y}$

---

${\mathbf{h}}_{0} \leftarrow  0$

为 $i \leftarrow  1$ 到 $\operatorname{LENGTH}\left( \mathbf{x}\right)$ 循环

	${\mathbf{h}}_{i} \leftarrow  g\left( {\mathbf{U}{\mathbf{h}}_{i - 1} + \mathbf{W}{\mathbf{x}}_{i}}\right)$
	
	${\mathbf{y}}_{i} \leftarrow  f\left( {\mathbf{{Vh}}}_{i}\right)$

返回 $y$

---

图 13.3 简单循环网络中的前向推断。矩阵 $\mathbf{U},\mathbf{V}$ 和 $\mathbf{W}$ 在时间上共享，而 $\mathbf{h}$ 和 $\mathbf{y}$ 的新值在每个时间步计算。

#### 13.1.2 训练

与前馈网络一样，我们使用训练集、损失函数和反向传播来获得调整这些循环网络权重所需的梯度。如图 13.2 所示，我们现在有 3 组需要更新的权重:W(从输入层到隐藏层的权重)、$\mathbf{U}$(从先前隐藏层到当前隐藏层的权重)，以及最后的 $\mathbf{V}$(从隐藏层到输出层的权重)。

图 13.4 强调了两个在前馈网络反向传播中不用担心的问题。首先，为了计算时间 $t$ 的输出的损失函数，我们需要时间 $t - 1$ 的隐藏层。其次，时间 $t$ 的隐藏层既影响时间 $t$ 的输出，也影响时间 $t + 1$ 的隐藏层(从而影响 $t + 1$ 的输出和损失)。由此可知，为了评估累积到 ${\mathbf{h}}_{t}$ 的误差，我们需要知道它对当前输出以及随后输出的影响。

![bo_d55a0kv7aajc73fvg5i0_15_236_317_1159_672_0.jpg](images/bo_d55a0kv7aajc73fvg5i0_15_236_317_1159_672_0.jpg)

图 13.4 将简单循环神经网络在时间上展开的示意图。网络层在每个时间步重新计算，而权重 $\mathbf{U},\mathbf{V}$ 和 $\mathbf{W}$ 在所有时间步共享。

将反向传播算法针对这种情况调整，会得到用于训练 RNN 权重的两遍算法。在第一遍，我们执行前向推断，计算 ${\mathbf{h}}_{t},{\mathbf{y}}_{t}$，在每个时间步累积损失，保存每步的隐藏层值以供下一步使用。在第二遍，我们逆序处理序列，边走边计算所需梯度，计算并保存每步向后传播时隐藏层使用的误差项。这种通用方法通常称为通过时间的反向传播(Werbos 1974，Rumelhart 等 1986，Werbos 1990)。

幸运的是，借助现代计算框架和充足的计算资源，训练 RNN 无需特殊方法。如图 13.4 所示，将循环网络显式展开为前馈计算图可以消除任何显式递归，从而直接训练网络权重。在这种方法中，我们提供一个模板，指定网络的基本结构，包括输入、输出和隐藏层所需的所有参数、权重矩阵，以及要使用的激活和输出函数。然后，在给定具体输入序列时，我们可以生成针对该输入的展开前馈网络，并使用该计算图通过普通反向传播进行前向推断或训练。

对于涉及更长输入序列的应用，例如语音识别、字符级处理或流式连续输入，展开整个输入序列可能不可行。在这些情况下，我们可以将输入展开为可管理的固定长度片段，并将每个片段视为独立的训练项。

---

backpropaga-time

---

### 13.2 将 RNN 用作语言模型

让我们看看如何将 RNN 应用于语言建模任务。回想第3章，语言模型在给定前文上下文时预测序列中的下一个词。例如，如果前文是“Thanks for all the”，想知道下一个词是“fish”的概率，我们将计算:

$$
P\left( {\text{ fish } \mid  \text{ Thanks for all the }}\right)
$$

语言模型能为每个可能的下一个词分配这样的条件概率，从而给出整个词汇表的分布。我们也可以通过链式法则将这些条件概率组合起来，为整个序列分配概率:

$$
P\left( {w}_{1 : n}\right)  = \mathop{\prod }\limits_{{i = 1}}^{n}P\left( {{w}_{i} \mid  {w}_{ < i}}\right)
$$

第3章的 n-gram 语言模型根据与前 $n - 1$ 个词同时出现的计数来计算某词的概率。因此上下文大小为 $n - 1$ 。第6章的前馈语言模型的上下文则是窗口大小。

RNN 语言模型(Mikolov 等，2010)一次处理输入序列中的一个词，试图根据当前词和先前的隐藏状态预测下一个词。因此 RNN 不像 n-gram 模型那样存在有限上下文问题，也不像前馈语言模型那样具有固定上下文，因为隐藏状态原则上可以表示从序列开头到当前为止所有前文的信息。图13.5 勾画了 FFN 语言模型与 RNN 语言模型之间的差异，显示 RNN 语言模型使用上一步的隐藏状态 ${h}_{t - 1}$ 作为过去上下文的表示。

![bo_d55a0kv7aajc73fvg5i0_16_547_1356_906_429_0.jpg](images/bo_d55a0kv7aajc73fvg5i0_16_547_1356_906_429_0.jpg)

图13.5 简化示意了两种语言模型架构在文本中移动时的情况，展示了三个标记的示意上下文:(a)具有固定上下文并输入到权重矩阵 $\mathbf{W}$ 的前馈神经语言模型；(b)其中隐藏状态 ${\mathbf{h}}_{t - 1}$ 总结先前上下文的 RNN 语言模型。

#### 13.2.1 RNN 语言模型中的前向推理

循环语言模型的前向推理完全按照第13.1.1节的描述进行。输入序列 $\mathbf{X} = \left\lbrack  {{\mathbf{x}}_{1};\ldots ;{\mathbf{x}}_{t};\ldots ;{\mathbf{x}}_{N}}\right\rbrack$ 由一系列词组成，每个词表示为大小为 $\left| V\right|  \times  1$ 的独热向量，输出预测 $\widehat{\mathbf{y}}$ 是表示词汇表上概率分布的向量。在每一步，模型使用词嵌入矩阵 $\mathbf{E}$ 检索当前词的嵌入，将其与权重矩阵 $\mathbf{W}$ 相乘，然后把它与前一步的隐藏层(经权重矩阵 $\mathbf{U}$ 加权)相加以计算新的隐藏层。该隐藏层随后用于生成输出层，再经 softmax 层得到整个词汇表上的概率分布。也就是说，在时间步 $t$:

$$
{\mathbf{e}}_{t} = \mathbf{E}{\mathbf{x}}_{t} \tag{13.4}
$$

$$
{\mathbf{h}}_{t} = g\left( {{\mathbf{{Uh}}}_{t - 1} + {\mathbf{{We}}}_{t}}\right) \tag{13.5}
$$

$$
{\widehat{\mathbf{y}}}_{t} = \operatorname{softmax}\left( {\mathbf{{Vh}}}_{t}\right) \tag{13.6}
$$

当我们用 RNN 做语言建模时(在第8章用 transformers 时我们会再次看到)，通常假设嵌入维度 ${d}_{e}$ 和隐藏维度 ${d}_{h}$ 相同。因此我们把两者都称为模型维度 $d$ 。所以嵌入矩阵 $\mathbf{E}$ 的形状是 $\left\lbrack  {d \times  \left| V\right| }\right\rbrack$，而 ${\mathbf{x}}_{\mathbf{t}}$ 是形状为 $\left\lbrack  {\left| V\right|  \times  1}\right\rbrack$ 的独热向量。乘积 ${\mathbf{e}}_{t}$ 因此是形状为 $\left\lbrack  {d \times  1}\right\rbrack  .\mathbf{W}$，且 $\mathbf{U}$ 的形状为 $\left\lbrack  {d \times  d}\right\rbrack$，因此 ${\mathbf{h}}_{t}$ 的形状也是 $\left\lbrack  {d \times  1}\right\rbrack  .\mathbf{V}$ 是形状为 $\left\lbrack  {\left| V\right|  \times  d}\right\rbrack$，所以 $\mathbf{{Vh}}$ 的结果是一个形状为 $\left\lbrack  {\left| V\right|  \times  1}\right\rbrack$ 的向量。该向量可以被视为在给定 $\mathbf{h}$ 提供的证据下对词汇表的得分集合。将这些得分通过 softmax 后会把得分归一化为一个概率分布。词汇表中某个特定单词 $k$ 是下一个单词的概率由 ${\widehat{\mathbf{y}}}_{t}\left\lbrack  k\right\rbrack$ 表示，即 ${\widehat{\mathbf{y}}}_{t}$ 的第 $k$ 个分量:

$$
P\left( {{w}_{t + 1} = k \mid  {w}_{1},\ldots ,{w}_{t}}\right)  = {\widehat{\mathbf{y}}}_{t}\left\lbrack  k\right\rbrack \tag{13.7}
$$

整个序列的概率就是序列中每个项概率的乘积，我们用 ${\widehat{\mathbf{y}}}_{i}\left\lbrack  {w}_{i}\right\rbrack$ 表示在时间步 $i$ 真实单词 ${w}_{i}$ 的概率。

$$
P\left( {w}_{1 : n}\right)  = \mathop{\prod }\limits_{{i = 1}}^{n}P\left( {{w}_{i} \mid  {w}_{1 : i - 1}}\right) \tag{13.8}
$$

$$
= \mathop{\prod }\limits_{{i = 1}}^{n}{\widehat{\mathbf{y}}}_{i}\left\lbrack  {w}_{i}\right\rbrack \tag{13.9}
$$

#### 13.2.2 训练 RNN 语言模型

---

自监督

---

为了训练一个 RNN 作为语言模型，我们使用第7.5节看到的相同自监督(或自训练)算法:以文本语料作为训练材料，在每个时间步 $t$ 要求模型预测下一个单词。我们称这样的模型为自监督的，因为无需向数据添加任何特殊的人工标签；词语的自然序列本身就是监督信号！我们仅训练模型以最小化预测训练序列中真实下一个单词的误差，使用交叉熵作为损失函数。回想交叉熵损失衡量的是预测的概率分布与正确分布之间的差异。

$$
{L}_{CE} =  - \mathop{\sum }\limits_{{w \in  V}}{\mathbf{y}}_{t}\left\lbrack  w\right\rbrack  \log {\widehat{\mathbf{y}}}_{t}\left\lbrack  w\right\rbrack \tag{13.10}
$$

在语言建模中，正确分布 ${\mathbf{y}}_{t}$ 来自于已知的下一个单词。这表示为对应词汇表的独热向量，其中真实下一个单词的位置为1，其他位置为0。因此，语言建模的交叉熵损失由模型给正确下一个单词分配的概率决定。所以在时间步 $t$ 的 CE 损失是模型对训练序列中下一个单词所分配概率的负对数。

$$
{L}_{CE}\left( {{\widehat{\mathbf{y}}}_{t},{\mathbf{y}}_{t}}\right)  =  - \log {\widehat{\mathbf{y}}}_{t}\left\lbrack  {w}_{t + 1}\right\rbrack \tag{13.11}
$$

![bo_d55a0kv7aajc73fvg5i0_18_277_315_1180_547_0.jpg](images/bo_d55a0kv7aajc73fvg5i0_18_277_315_1180_547_0.jpg)

图13.6 将 RNN 作为语言模型进行训练。

---

教师强制

---

因此，在输入的每个词位点 $t$ ，模型将正确的词 ${w}_{t}$ 与 ${h}_{t - 1}$ 一并作为输入，编码来自先前 ${w}_{1 : t - 1}$ 的信息，并利用它们计算可能下一个词的概率分布，从而计算下一个标记的模型损失 ${w}_{t + 1}$ 。然后移动到下一个词，我们忽略模型对下一个词的预测，而是使用正确的词 ${w}_{t + 1}$ 及先前编码的历史来估计标记 ${w}_{t + 2}$ 的概率。我们总是给模型正确的历史序列来预测下一个词(而不是将上一步的最优输出喂回模型)的做法称为教师强制。

通过梯度下降调整网络中的权重以最小化整个训练序列上的平均交叉熵损失。图 13.6 说明了该训练方案。

#### 13.2.3 权重绑定

细心的读者可能已经注意到，输入嵌入矩阵 $\mathbf{E}$ 与馈入输出 softmax 的最终层矩阵 $\mathbf{V}$ 非常相似。

$\mathbf{E}$ 的列表示在训练过程中学习到的词汇表中每个词的词嵌入，目标是使具有相似意义和功能的词具有相似的嵌入。并且，由于在用 RNN 做语言建模时我们假设嵌入维度与隐藏维度相同(= 模型维度 $d$ )，嵌入矩阵 $\mathbf{E}$ 的形状为 $\left\lbrack  {d \times  \left| V\right| }\right\rbrack$ 。最终层矩阵 $\mathbf{V}$ 提供了一种途径，通过计算 $\mathbf{{Vh}}.\mathbf{V}$ 来根据网络最终隐藏层中的证据为词汇表中每个词评分，其形状为 $\left\lbrack  {\left| V\right|  \times  d}\right\rbrack$ 。也就是说，$\mathbf{V}$ 的行的形状类似于 $\mathbf{E}$ 的转置，这意味着 $\mathbf{V}$ 提供了第二组学习到的词嵌入。

---

权重绑定

---

语言模型不是用两套嵌入矩阵，而是使用一套出现在输入和 softmax 层的单一嵌入矩阵。也就是说，我们省去 $\mathbf{V}$，在计算开始时使用 $\mathbf{E}$，并在末端使用 ${\mathbf{E}}^{\top }$(因为 $\mathbf{V}$ 的形状是 $\mathbf{E}$ 的转置)。在两个位置使用相同的(转置的)矩阵称为权重绑定。${}^{1}$ 于是 RNN 语言模型的权重绑定方程变为:

$$
{\mathbf{e}}_{t} = \mathbf{E}{\mathbf{x}}_{t} \tag{13.12}
$$

$$
{\mathbf{h}}_{t} = g\left( {{\mathbf{{Uh}}}_{t - 1} + {\mathbf{{We}}}_{t}}\right) \tag{13.13}
$$

$$
{\widehat{\mathbf{y}}}_{t} = \operatorname{softmax}\left( {{\mathbf{E}}^{\top }{\mathbf{h}}_{t}}\right) \tag{13.14}
$$

除了能提升模型困惑度外，这种方法还显著减少了模型所需的参数数量。

### 13.3 RNN 在其他 NLP 任务中的应用

现在我们已经看过基本的 RNN 架构，来考虑如何将其应用于三类 NLP 任务:诸如情感分析和主题分类的序列分类任务、诸如词性标注的序列标注任务，以及包括一种名为编码器-解码器的新架构在内的文本生成任务。

#### 13.3.1 序列标注

在序列标注中，网络的任务是为序列中的每个元素分配从一小集合标签中选择的标签。一个经典的序列标注任务是词性(POS)标注(为句中每个词分配诸如 NOUN 和 VERB 的语法标签)。我们将在第 17 章详细讨论词性标注，但这里给出一个启发性示例。在 RNN 的序列标注方法中，输入是词嵌入，输出是通过给定标签集上的 softmax 层生成的标签概率，如图 13.7 所示。

在此图中，每个时间步的输入是与输入标记对应的预训练词向量。RNN 模块是对展开的简单递归网络的抽象表示，该网络在每个时间步包含输入层、隐藏层和输出层，以及构成网络的共享 $\mathbf{U},\mathbf{V}$ 与 $\mathbf{W}$ 权重矩阵。网络在每个时间步的输出表示通过 softmax 层产生的关于词性标签集的分布。

为了为给定输入生成标签序列，我们对输入序列进行前向推断，并在每个时间步从 softmax 中选择最可能的标签。由于我们在每个时间步使用 softmax 层来生成输出标签集的概率分布，训练时仍将采用交叉熵损失。

#### 13.3.2 用于序列分类的 RNN

RNN 的另一个用途是对整个序列而非序列内的标记进行分类。这类任务通常称为文本分类，例如情感分析或垃圾邮件检测，我们将文本分类到两类或三类(如正面或负面)，也包括类别众多的分类任务，如文档级主题分类或客户服务应用中的消息路由。

---

1 我们在 Transformer(第 8 章)中也这样做，通常将 ${\mathbf{E}}^{\top }$ 称为反嵌入矩阵。

---

![bo_d55a0kv7aajc73fvg5i0_20_468_316_986_512_0.jpg](images/bo_d55a0kv7aajc73fvg5i0_20_468_316_986_512_0.jpg)

图 13.7 将词性标注作为使用简单 RNN 的序列标注。词性(POS)标注的目标是为句子中的每个单词分配一个来自预定义标签集的语法标签。(本句的标签包括 NNP(专有名词)、MD(情态动词)等；我们将在第 17 章给出词性标注任务的完整描述。)预训练的词嵌入作为输入，softmax 层在每个时间步为词性标签提供概率分布作为输出。

要在此场景中应用 RNN，我们将待分类的文本逐词传入 RNN，在每个时间步生成新的隐藏层表示。然后我们可以取文本最后一个标记的隐藏层 ${\mathbf{h}}_{n}$ 作为整个序列的压缩表示。我们可以将该表示 ${\mathbf{h}}_{n}$ 传入一个前馈网络，该网络通过对可能类别做 softmax 来选择类别。图 13.8 说明了这种方法。

![bo_d55a0kv7aajc73fvg5i0_20_470_1366_983_446_0.jpg](images/bo_d55a0kv7aajc73fvg5i0_20_470_1366_983_446_0.jpg)

图 13.8 使用简单 RNN 结合前馈网络的序列分类。RNN 的最终隐藏状态被用作前馈网络的输入，该网络执行分类。

注意，在这种方法中我们不需要为序列中最后元素之前的单词产生中间输出。因此，与那些元素没有关联的损失项。相反，用于训练网络权重的损失函数完全基于最终的文本分类任务。来自前馈分类器的 softmax 输出与交叉熵损失共同驱动训练。来自分类的误差信号被反向传播，通过前馈分类器的权重传回到其输入，再传回到 RNN 的三组权重，如第 13.1.2 节之前所述。使用下游任务损失来调整整个网络权重的训练方式称为端到端训练。

另一种选择，不是仅用最后一个标记的隐藏状态 ${h}_{n}$ 来表示整个序列，而是对序列中每个单词的所有隐藏状态 ${h}_{i}$ 使用某种池化函数 $i$。例如，我们可以通过对所有 $n$ 隐藏状态取逐元素均值来创建一个池化表示:

$$
{\mathbf{h}}_{\text{ mean }} = \frac{1}{n}\mathop{\sum }\limits_{{i = 1}}^{n}{\mathbf{h}}_{i} \tag{13.15}
$$

或者我们可以取按元素最大值；一组 $n$ 向量的按元素最大值是一个新向量，其第 $k$ 个元素是所有这些 $n$ 向量第 $k$ 个元素的最大值。

RNN 的长上下文使得将误差成功反向传播穿过整个输入变得相当困难；我们将在第 13.5 节讨论这个问题以及一些常见的解决方法。

#### 13.3.3 基于 RNN 的语言模型的生成

基于 RNN 的语言模型也可用于生成文本。文本生成与图像生成和代码生成一道，构成了常称为生成式 AI 的新领域。已经读过第 7 章和第 8 章的读者可能已经见过这些内容，但我们在此对按不同顺序阅读的读者做重述。

回想在第 3 章我们看到如何通过改编 Claude Shannon(Shannon, 1951)与心理学家 George Miller 和 Jennifer Selfridge(Miller 和 Selfridge, 1950)几乎同时提出的一种采样技术，从 n 元语法模型生成文本。我们首先根据一个词作为序列开头的合适性随机采样一个单词开始序列，然后继续在已选择的前提下采样后续单词，直到达到预定长度或生成序列结束标记。

如今，这种使用语言模型通过反复在先前选择条件下采样下一个词来增量生成词语的方法称为自回归生成或因果语言模型生成。该过程基本与第 48 页所述相同，但已适配到神经网络环境:

- 从以句首标记 <s> 作为第一个输入得到的 softmax 分布中采样一个词作为输出。

- 将该首词的词嵌入作为下一时间步网络的输入，然后以相同方式采样下一个词。

- 继续生成直到采样到句末标记 </s> 或达到固定长度上限。

从技术上讲，自回归模型是在时间 $t$ 基于前一时间 $t - 1, t - 2$ 的值的线性函数预测值的模型。尽管语言模型不是线性的(因为它们有许多非线性层)，我们仍松散地将这种生成技术称为自回归生成，因为每一时间步生成的词都以网络在上一步选择的词为条件。图 13.9 说明了这种方法。在该图中，RNN 的隐藏层和循环连接的细节被隐藏在蓝色块中。

---

端到端训练

池化

自回归生成

---

这种简单架构是机器翻译、摘要和问答等应用中最先进方法的基础。关键在于用适当的上下文预置生成组件。也就是说，我们可以提供更丰富、适合任务的上下文来替代仅用 <s> 启动生成；对于翻译，上下文是源语言句子；对于摘要，是我们要摘要的长文本。

![bo_d55a0kv7aajc73fvg5i0_22_479_568_970_517_0.jpg](images/bo_d55a0kv7aajc73fvg5i0_22_479_568_970_517_0.jpg)

图 13.9 基于 RNN 的神经语言模型的自回归生成。

### 13.4 堆叠与双向 RNN 架构

循环网络非常灵活。通过将展开的计算图的前馈特性与向量作为通用输入输出结合，复杂网络可作为模块以创造性方式组合。本节介绍两种在 RNN 语言处理里较常用的网络架构。

#### 13.4.1 堆叠 RNN

---

堆叠 RNN

---

到目前为止，我们的示例中 RNN 的输入由词或字符嵌入(向量)序列构成，输出为用于预测词、标签或序列标注的向量。然而，没有什么阻止我们把一个 RNN 的整个输出序列作为另一个 RNN 的输入序列。堆叠 RNN 由多层网络组成，其中一层的输出作为后续层的输入，如图 13.10 所示。

堆叠 RNN 通常优于单层网络。其成功的一个原因似乎是网络在不同层诱导出不同抽象层次的表示。正如人类视觉系统的早期阶段检测边缘用于发现更大区域和形状，堆叠网络的初始层可以诱导出供后续层使用的有用抽象——这些表示可能难以在单个 RNN 中诱导得到。最佳堆叠层数取决于具体应用和训练集。但随着堆叠数量增加，训练成本会迅速上升。

![bo_d55a0kv7aajc73fvg5i0_23_412_312_990_495_0.jpg](images/bo_d55a0kv7aajc73fvg5i0_23_412_312_990_495_0.jpg)

图 13.10 堆叠循环网络。低层的输出作为高层的输入，最后一层网络的输出作为最终输出。

#### 13.4.2 双向 RNN

RNN 在时间 $t$ 使用来自左侧(先前)上下文的信息来进行预测。但在许多应用中我们可以获得整个输入序列；在这些情况下我们希望利用位于 $t$ 右侧的上下文单词。一种方法是运行两个独立的 RNN，一个从左到右，一个从右到左，然后将它们的表示拼接起来。

在到目前为止讨论的从左到右的 RNN 中，给定时间 $t$ 的隐藏状态表示网络到该点为止关于序列所知道的一切。该状态是输入 ${x}_{1},\ldots ,{x}_{t}$ 的函数，表示当前时间左侧的网络上下文。

$$
{\mathbf{h}}_{t}^{f} = {\operatorname{RNN}}_{\text{ forward }}\left( {{\mathbf{x}}_{1},\ldots ,{\mathbf{x}}_{t}}\right) \tag{13.16}
$$

这个新的符号 ${\mathbf{h}}_{t}^{f}$ 简单地对应于时间 $t$ 的常规隐藏状态，表示网络迄今从序列中获取的一切信息。

为利用当前输入右侧的上下文，我们可以在反转的输入序列上训练一个 RNN。采用这种方法，时间 $t$ 的隐藏状态表示关于当前输入右侧序列的信息:

$$
{\mathbf{h}}_{t}^{b} = {\operatorname{RNN}}_{\text{ backward }}\left( {{\mathbf{x}}_{t},\ldots {\mathbf{x}}_{n}}\right) \tag{13.17}
$$

在这里，隐藏状态 ${\mathbf{h}}_{t}^{b}$ 表示我们从 $t$ 到序列末端所辨别出的所有信息。

双向 RNN(Schuster 和 Paliwal，1997)将两个独立的 RNN 结合起来，一个从开始处理到结尾，另一个从结尾处理到开始。然后我们将两个网络计算出的表示拼接成一个向量，捕捉输入在每个时间点的左右上下文。在此我们使用分号 ";" 或等价符号 $\oplus$ 表示向量拼接:

$$
{\mathbf{h}}_{t} = \left\lbrack  {{\mathbf{h}}_{t}^{f};{\mathbf{h}}_{t}^{b}}\right\rbrack
$$

$$
= {\mathbf{h}}_{t}^{f} \oplus  {\mathbf{h}}_{t}^{b} \tag{13.18}
$$

---

双向 RNN

---

图 13.11 展示了这样一个将前向和后向输出拼接的双向网络。结合前后文的其他简单方法包括逐元素相加或相乘。每个时间步的输出因此同时包含当前输入的左侧和右侧信息。在序列标注应用中，这些拼接的输出可以作为局部标注决策的依据。

![bo_d55a0kv7aajc73fvg5i0_24_466_564_990_543_0.jpg](images/bo_d55a0kv7aajc73fvg5i0_24_466_564_990_543_0.jpg)

图 13.11 一个双向 RNN。分别在前向和后向方向训练模型，将每个时间点每个模型的输出拼接以表示该时间点的双向状态。

双向 RNN 在序列分类上也被证明非常有效。回想图 13.8，对于序列分类我们使用 RNN 的最终隐藏状态作为后续前馈分类器的输入。这种方法的一个难点是最终状态自然更多反映句子结尾的信息而非开头。双向 RNN 为此提供了一个简单的解决方案；如图 13.12 所示，我们只需将前向和后向通过的最终隐藏状态(例如通过拼接)结合起来，并将其作为后续处理的输入。

### 13.5 LSTM

在实践中，对于需要网络利用远离当前处理点的信息的任务，训练 RNN 是相当困难的。尽管可以访问整个先前序列，但隐藏状态中编码的信息往往相当局部，更与输入序列的最近部分和最近的决策相关。然而远距离信息对许多语言应用至关重要。考虑以下语言建模上下文的示例。

(13.19) 航空公司正在取消的航班都已满员。

在 airline 之后给 was 赋予高概率是直接的，因为 airline 在局部提供了强烈的单数一致性。然而，为 were 赋予适当概率相当困难，不仅因为复数名词 flights 相当遥远，而且因为单数名词 airline 在中间上下文中更接近。理想情况下，网络应能保留有关复数 flights 的远距离信息直到需要为止，同时仍然正确处理序列的中间部分。

![bo_d55a0kv7aajc73fvg5i0_25_412_311_992_650_0.jpg](images/bo_d55a0kv7aajc73fvg5i0_25_412_311_992_650_0.jpg)

图 13.12 用于序列分类的双向 RNN。来自前向和后向通过的最终隐藏单元被结合以表示整个序列。该组合表示作为后续分类器的输入。

RNN 无法将关键信息传递下去的一个原因是，隐藏层及决定隐藏层数值的权重同时被要求执行两项任务:为当前决策提供有用信息，以及更新并携带供未来决策所需的信息。

训练 RNN 的第二个困难来自必须将误差信号反向传播回时间。回想第 13.1.2 节，时间 $t$ 的隐藏层会影响下一时间步的损失，因为它参与了该计算。因此，在训练的反向传播过程中，隐藏层会按序列长度反复相乘。这一过程的常见结果是梯度最终被驱向零，称为梯度消失问题。

为了解决这些问题，提出了更复杂的网络架构，明确管理随时间保持相关上下文的任务，使网络学会遗忘不再需要的信息并记住将来决策所需的信息。

最常用的此类 RNN 扩展是长短期记忆(LSTM)网络(Hochreiter 和 Schmidhuber，1997)。LSTM 将上下文管理问题分为两部分:从上下文中移除不再需要的信息，以及向上下文中添加可能在后续决策中需要的信息。解决这两类问题的关键是学习如何管理上下文，而不是将策略写死在架构中。LSTM 通过在架构中增加显式的上下文层(除了常规的循环隐藏层之外)，并使用特殊的神经单元通过门控控制信息进出网络层的单元来实现这一点。这些门通过额外权重实现，顺序作用于输入、先前隐藏层和先前上下文层。

---

梯度消失

长短期记忆

---

LSTM 中的门具有共同的设计模式；每个门由一个前馈层、一个 sigmoid 激活函数，随后与被门控的层进行逐点相乘组成。选择 sigmoid 作为激活函数是因为它倾向于将输出推向 0 或 1。将其与逐点相乘结合的效果类似于二进制掩码。被门控层中对应掩码中接近 1 的值几乎不变地通过；对应较小值的元素则基本被抹去。

我们要考虑的第一个门是遗忘门。该门的目的是从上下文中删除不再需要的信息。遗忘门对前一状态的隐藏层和当前输入计算加权和并通过 sigmoid。然后将该掩码与上下文向量逐元素相乘，以从上下文中移除不再需要的信息。两个向量的逐元素相乘(由运算符 $\odot$ 表示，有时称为 Hadamard 乘积)是与两个输入向量维度相同的向量，其中每个元素 $i$ 是两个输入向量中对应元素 $i$ 的乘积:

$$
{\mathbf{f}}_{t} = \sigma \left( {{\mathbf{U}}_{f}{\mathbf{h}}_{t - 1} + {\mathbf{W}}_{f}{\mathbf{x}}_{t}}\right) \tag{13.20}
$$

$$
{\mathbf{k}}_{t} = {\mathbf{c}}_{t - 1} \odot  {\mathbf{f}}_{t} \tag{13.21}
$$

接下来的任务是计算我们需要从先前隐藏状态和当前输入中提取的实际信息—这与我们为所有循环网络一直使用的基本计算相同。

$$
{\mathbf{g}}_{t} = \tanh \left( {{\mathbf{U}}_{g}{\mathbf{h}}_{t - 1} + {\mathbf{W}}_{g}{\mathbf{x}}_{t}}\right) \tag{13.22}
$$

接着，我们为添加门生成掩码以选择要添加到当前上下文的信息。

$$
{\mathbf{i}}_{t} = \sigma \left( {{\mathbf{U}}_{i}{\mathbf{h}}_{t - 1} + {\mathbf{W}}_{i}{\mathbf{x}}_{t}}\right) \tag{13.23}
$$

$$
{\mathbf{j}}_{t} = {\mathbf{g}}_{t} \odot  {\mathbf{i}}_{t} \tag{13.24}
$$

然后，我们将其加到修改后的上下文向量上以得到新的上下文向量。

$$
{\mathbf{c}}_{t} = {\mathbf{j}}_{t} + {\mathbf{k}}_{t} \tag{13.25}
$$

我们将使用的最后一个门是输出门，用于决定当前隐藏状态所需的信息(与需要为未来决策保留的信息相对)。

$$
{\mathbf{o}}_{t} = \sigma \left( {{\mathbf{U}}_{o}{\mathbf{h}}_{t - 1} + {\mathbf{W}}_{o}{\mathbf{x}}_{t}}\right) \tag{13.26}
$$

$$
{\mathbf{h}}_{t} = {\mathbf{o}}_{t} \odot  \tanh \left( {\mathbf{c}}_{t}\right) \tag{13.27}
$$

图 13.13 展示了单个 LSTM 单元的完整计算。在各门具有适当权重的情况下，LSTM 接受来自前一时间步的上下文层和隐藏层以及当前输入向量作为输入，然后生成更新后的上下文和隐藏向量作为输出。

正是隐藏状态 ${h}_{t}$ 为 LSTM 在每个时间步提供输出。该输出可作为堆叠 RNN 后续层的输入，或在网络的最终层中 ${h}_{t}$ 可用于提供 LSTM 的最终输出。

---

遗忘门

输出门

---

![bo_d55a0kv7aajc73fvg5i0_27_420_320_782_481_0.jpg](images/bo_d55a0kv7aajc73fvg5i0_27_420_320_782_481_0.jpg)

图 13.13 将单个 LSTM 单元显示为计算图。每个单元的输入包括当前输入 $x$、上一个隐藏状态 ${h}_{t - 1}$ 和上一个上下文 ${c}_{t - 1}$。输出为新的隐藏状态 ${h}_{t}$ 和更新后的上下文 ${c}_{t}$。

![bo_d55a0kv7aajc73fvg5i0_27_535_943_744_430_0.jpg](images/bo_d55a0kv7aajc73fvg5i0_27_535_943_744_430_0.jpg)

图 13.14 在前馈网络、简单循环网络(SRN)和长短期记忆(LSTM)中使用的基本神经单元。

#### 13.5.1 门控单元、层与网络

LSTM 中使用的神经单元显然比基本前馈网络中的要复杂得多。幸运的是，这种复杂性被封装在基本处理单元中，使我们能够保持模块化，并轻松试验不同的架构。为说明此点，参见图 13.14，它展示了每种单元的输入和输出。

最左侧的 (a) 是基本前馈单元，其中一组权重和一个激活函数决定其输出，且当排列成一层时，层内单元之间没有连接。接着 (b) 表示简单循环网络中的单元。现在有两个输入和一组额外的权重。但仍只有一个激活函数和一个输出。

LSTM 单元的复杂性被封装在单元内部。与基本循环单元 (b) 相比，LSTM 唯一额外的外部复杂性是作为输入和输出的额外上下文向量的存在。

这种模块化是 LSTM 单元强大且广泛适用的关键。LSTM 单元(或其他变体，如 GRU)可以替换到第 13.4 节描述的任何网络架构中。同简单 RNN 一样，使用门控单元的多层网络可以展开成深度前馈网络，并以常规方式用反向传播训练。因此在实践中，LSTM 而非 RNN 已成为任何使用循环网络的现代系统的标准单元。

### 13.6 小结:常见的 RNN 自然语言处理架构

我们现在介绍了 RNN，看到诸如堆叠多层和使用 LSTM 版本等进阶组件，并了解了 RNN 可如何应用于各种任务。让我们花点时间总结这些应用的架构。

图 13.15 展示了我们迄今讨论的三种架构:序列标注、序列分类和语言模型。在序列标注(例如词性标注)中，我们训练模型为每个输入单词或标记产生一个标签。在序列分类中，例如情感分析，我们忽略每个标记的输出，仅取序列末尾的值(模型的训练信号同样来自最后一个标记的反向传播)。在语言建模中，我们训练模型在每个标记步骤预测下一个单词。下一节我们将介绍第四种架构:编码器—解码器。

![bo_d55a0kv7aajc73fvg5i0_28_276_1109_1180_724_0.jpg](images/bo_d55a0kv7aajc73fvg5i0_28_276_1109_1180_724_0.jpg)

图 13.15 四种用于 NLP 任务的架构。在序列标注(词性或命名实体标注)中，我们将每个输入标记 ${x}_{i}$ 映射为一个输出标记 ${y}_{i}$。在序列分类中，我们将整个输入序列映射为一个类别。在语言建模中，我们在先前标记的条件下输出下一个标记。在编码器模型中，我们有两个独立的 RNN 模型，其中一个将输入序列 $\mathbf{x}$ 映射为我们称之为上下文的中间表示，另一个将上下文映射为输出序列 $\mathbf{y}$。

### 13.7 使用 RNN 的编码器—解码器模型

本节介绍编码器—解码器模型，当我们将输入序列翻译为与输入长度不同且不逐词对齐的输出序列时会使用该模型。

已经阅读第 12 章的读者会在 Transformer 架构及其在机器翻译中的应用中见过该模型，但我们在此再次介绍该架构，以便那些先学 RNN 再学 Transformer 的读者也能理解。

回想在序列标注任务中，我们有两条序列，但它们长度相同(例如在词性标注中每个标记都有一个对应标签)，每个输入与特定输出相关联，且该输出的标注主要依赖局部信息。因此在判断一个词是动词还是名词时，我们主要看该词及其相邻词。

相比之下，编码器-解码器模型尤其用于机器翻译等任务，其中输入序列和输出序列长度可以不同，输入中某个标记与输出中某个标记的对应可以非常间接(在某些语言中动词出现在句首；在其他语言出现在句尾)。我们在第12章介绍了机器翻译，但这里先指出，从英语句子到塔加洛语或约鲁巴语的映射可能包含非常不同数量的单词，且单词顺序可能大相径庭。

---

编码器-解码器

---

编码器-解码器网络，有时称为序列到序列网络，是在给定输入序列的情况下能够生成上下文适切的任意长度输出序列的模型。编码器-解码器网络已被应用于摘要、问答和对话等广泛任务，但在机器翻译中特别流行。

这些网络的核心思想是使用一个编码器网络接受输入序列并创建其上下文化表示，通常称为上下文。该表示随后传递给解码器以生成特定任务的输出序列。图13.16说明了该架构。

![bo_d55a0kv7aajc73fvg5i0_29_420_1474_984_317_0.jpg](images/bo_d55a0kv7aajc73fvg5i0_29_420_1474_984_317_0.jpg)

图13.16 编码器-解码器架构。上下文是输入隐藏表示的函数，并可被解码器以多种方式使用。

编码器-解码器网络由三个概念性组成部分构成:

1. 一个编码器，接受输入序列，${x}_{1 : n}$，并生成相应的上下文化表示序列，${h}_{1 : n}$。LSTM、卷积网络和变换器都可以用作编码器。

2. 一个上下文向量，$c$，它是${h}_{1 : n}$的函数，并将输入的要旨传达给解码器。

3. 一个解码器，接受$c$作为输入并生成任意长度的隐藏状态序列${h}_{1 : m}$，从中可以得到相应的输出状态序列${y}_{1 : m}$。与编码器类似，解码器可以由任何序列架构来实现。

在本节中我们将描述基于一对RNN的编码器-解码器网络，但在第12章我们也将看到如何将其应用于变换器。我们将从条件RNN语言模型$p\left( y\right)$出发构建编码器-解码器模型的方程，即序列$y$的概率。

回想在任何语言模型中，我们可以将概率分解如下:

$$
p\left( y\right)  = p\left( {y}_{1}\right) p\left( {{y}_{2} \mid  {y}_{1}}\right) p\left( {{y}_{3} \mid  {y}_{1},{y}_{2}}\right) \ldots p\left( {{y}_{m} \mid  {y}_{1},\ldots ,{y}_{m - 1}}\right) \tag{13.28}
$$

在RNN语言建模中，在特定时间$t$，我们将$t - 1$个标记的前缀传入语言模型，使用前向推断产生一系列隐藏状态，以与前缀最后一个词对应的隐藏状态结束。然后我们使用前缀的最终隐藏状态作为起点来生成下一个标记。

更正式地，如果$g$是如tanh或ReLU之类的激活函数，是时间$t$的输入和时间$t - 1$的隐藏状态的函数，且softmax是对可能词汇项集合的归一化，那么在时间$t$输出${\mathbf{y}}_{t}$和隐藏状态${\mathbf{h}}_{t}$按下式计算:

$$
{\mathbf{h}}_{t} = g\left( {{\mathbf{h}}_{t - 1},{\mathbf{x}}_{t}}\right) \tag{13.29}
$$

$$
{\widehat{\mathbf{y}}}_{t} = \operatorname{softmax}\left( {\mathbf{h}}_{t}\right) \tag{13.30}
$$

我们只需做一处微小修改，就可将这个带自回归生成的语言模型变为能够将一种语言的源文翻译为第二种语言目标文的编码器-解码器翻译模型:在源文末端添加句子分隔标记，然后简单地将目标文本拼接上去。

我们用<s>作为句子分隔符，并设想将英语源文(“the green witch arrived”)翻译为西班牙语句子(“llegó la bruja verde”(逐词可注释为“arrived the witch green”))。我们也可以用问答对或文本摘要对来说明编码器-解码器模型。

我们用$x$指代源文(在此为英语)加上分隔符<s>，并用$y$指代目标文本$y$(在此为西班牙语)。那么编码器-解码器模型按如下计算概率$p\left( {y \mid  x}\right)$:

$$
p\left( {y \mid  x}\right)  = p\left( {{y}_{1} \mid  x}\right) p\left( {{y}_{2} \mid  {y}_{1}, x}\right) p\left( {{y}_{3} \mid  {y}_{1},{y}_{2}, x}\right) \ldots p\left( {{y}_{m} \mid  {y}_{1},\ldots ,{y}_{m - 1}, x}\right) \tag{13.31}
$$

图13.17显示了简化版编码器-解码器模型的设置(完整版需要新的注意力概念，将在下一节介绍)。

图13.17显示了一个英文源文本("the green witch arrived")、一个句子分隔符令牌(<s>)和一个西班牙语目标文本("llegó la bruja verde")。为翻译源文本，我们将其输入网络，进行前向推理以生成隐藏状态，直到源句结束。然后开始自回归生成，在考虑源输入末端隐藏层以及句子结束标记的上下文下请求一个单词。后续单词则以先前的隐藏状态和最后生成单词的嵌入为条件。

---

句子分隔

---

![bo_d55a0kv7aajc73fvg5i0_31_224_318_1177_509_0.jpg](images/bo_d55a0kv7aajc73fvg5i0_31_224_318_1177_509_0.jpg)

图13.17 在基本RNN版编码器-解码器机器翻译方法中展示了单句翻译(推理时)。源句和目标句用分隔令牌连接，解码器使用来自编码器最后隐藏状态的上下文信息。

让我们在图13.18中对该模型稍作形式化和泛化。(为便于区分编码器和解码器的隐藏状态，我们在必要处使用上标$e$和$d$。)左侧网络的各元素处理输入序列$x$，构成编码器。尽管简化图仅示出编码器的单层网络，但堆叠架构才是常态，通常取堆栈最顶层的输出状态作为最终表示，编码器由堆叠的双向LSTM组成，来自前向和后向传递顶层的隐藏状态被拼接以为每个时间步提供上下文化的表示。

![bo_d55a0kv7aajc73fvg5i0_31_224_1325_1178_466_0.jpg](images/bo_d55a0kv7aajc73fvg5i0_31_224_1325_1178_466_0.jpg)

图13.18 在基本的基于RNN的编码器—解码器架构中，推理时翻译句子的更正式版本。编码器RNN的最终隐藏状态${h}_{n}^{e}$作为解码器中${h}_{0}^{d}$的上下文，同时也对每个解码器隐藏状态可用。

编码器的全部目的在于生成输入的上下文化表示。该表示体现在编码器的最终隐藏状态${\mathbf{h}}_{n}^{e}$中。这一表示，也称为上下文$\mathbf{c}$，随后被传递给解码器。

解码器网络最简单的版本会把该状态仅用于初始化解码器的第一个隐藏状态；第一个解码器RNN单元将把$c$用作其先前隐藏状态${\mathbf{h}}_{0}^{d}$。然后解码器会自回归地逐个生成输出元素，直到生成序列结束标记。每个隐藏状态都以先前的隐藏状态和先前步骤生成的输出为条件。

如图13.18所示，我们做得更复杂一些:我们使上下文向量$\mathbf{c}$不仅对第一个解码器隐藏状态可用，以确保上下文向量$\mathbf{c}$的影响在生成输出序列时不会减弱。我们通过将$\mathbf{c}$作为参数加入当前隐藏状态的计算来实现，使用以下方程:

$$
{\mathbf{h}}_{t}^{d} = g\left( {{\widehat{y}}_{t - 1},{\mathbf{h}}_{t - 1}^{d},\mathbf{c}}\right) \tag{13.32}
$$

现在我们准备查看在基本编码器—解码器模型中、在每个解码时刻都可用上下文的该版本解码器的完整方程。回想$g$代表某种RNN的替代，而${\widehat{y}}_{t - 1}$是上一步从softmax采样得到的输出的嵌入:

$$
\mathbf{c} = {\mathbf{h}}_{n}^{e}
$$

$$
{\mathbf{h}}_{0}^{d} = \mathbf{c}
$$

$$
{\mathbf{h}}_{t}^{d} = g\left( {{\widehat{y}}_{t - 1},{\mathbf{h}}_{t - 1}^{d},\mathbf{c}}\right)
$$

$$
{\widehat{\mathbf{y}}}_{t} = \operatorname{softmax}\left( {\mathbf{h}}_{t}^{d}\right) \tag{13.33}
$$

因此${\widehat{\mathbf{y}}}_{t}$是词表上概率的向量，表示每个单词在时间$t$出现的概率。为生成文本，我们从该分布${\widehat{\mathbf{y}}}_{t}$中采样。例如，贪心选择就是在每个时间步简单地选择最可能的单词生成。我们在第7.4节讨论了其他采样方法。

#### 13.7.1 编码器—解码器模型的训练

编码器—解码器架构进行端到端训练。每个训练示例是一对配对字符串:源语言和目标语言。与分隔符标记连接后，这些源-目标对即可作为训练数据。

对于机器翻译，训练数据通常由句子及其翻译构成。这些可以来自标准的对齐句子对数据集，如我们将在第12.2.2节讨论的。一旦我们有了训练集，训练过程与任何基于RNN的语言模型相同。网络接收源文本，然后从分隔符标记开始自回归地训练以预测下一个词，如图13.19所示。

注意训练(图13.19)与推理(图13.17)在每个时间步输出上的差异。推理时，解码器使用其自身估计的输出$\widehat{{y}_{t}}$作为下一时间步的输入${x}_{t + 1}$。因此，随着生成更多标记，解码器会越来越偏离真实目标句子。因此在训练中，更常用教师强制(teacher forcing)在解码器中。教师强制意味着我们强制系统使用训练中的真实目标标记作为下一步的输入${x}_{t + 1}$，而不是让它依赖(可能有误的)解码器输出${\widehat{y}}_{t}$。这加快了训练。

---

teacher forcing

---